<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Ayan</title>
  
  <meta name="author" content="Ayan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ayan Kumar Bhunia</name>
              </p>

              <p style="text-align:justify"><font size="3">
                I am a Doctor of Philosophy (PhD) student, focusing on Computer Vision and Deep Learning,
                at <a href="http://sketchx.eecs.qmul.ac.uk/" target="_blank"><font size="3">SketchX Lab.</font></a> of
                <a href="https://www.surrey.ac.uk/centre-vision-speech-signal-processing"target=" _blank"><font size="3">Centre for Vision,
                  Speech and Signal Processing (CVSSP)</font></a>, <a href="https://www.surrey.ac.uk/" target="_blank"><font size="3">
                University of Surrey</font></a>, England, United Kingdom.
                My primary supervisor is <a href="http://personal.ee.surrey.ac.uk/Personal/Y.Song/" target="_blank"><font size="3">
                Dr. Yi-Zhe Song</font></a>, and co-supervisors are
                <a href="http://personal.ee.surrey.ac.uk/Personal/T.Xiang/index.html" target="_blank">
                  <font size="3">Prof. Tao(Tony) Xiang</font></a> and <a href="https://www.surrey.ac.uk/people/yongxin-yang" target="_blank">
                  <font size="3">Dr. Yongxin Yang</font></a>.
                </font></p>
                <p align="justify"><font size="3">
                Prior to that, I worked as a research assistant at the
                  <a href="https://imi.ntu.edu.sg/Pages/Home.aspx"
                     target="_blank"><font size="3">Institute for Media Innovation (IMI) Lab</font></a> of
                  <a href="https://www.ntu.edu.sg/Pages/home.aspx"
                     target="_blank"><font size="3">Nanyang Technological University (NTU)</font></a>, Singapore.
                </font></p>
              <p style="text-align:center">
                <a href="https://scholar.google.co.in/citations?user=gjslbzsAAAAJ&hl=en" target="_blank"><font size="3">Google Scholar </font></a> &nbsp/&nbsp
                 <a href="https://github.com/AyanKumarBhunia" target="_blank"><font size="3"> GitHub </font></a> &nbsp/&nbsp
               <a href="https://www.linkedin.com/in/loansurrey/" target="_blank"><font size="3">  LinkedIn  </font></a> &nbsp/&nbsp
              <a href="https://dblp.org/pers/b/Bhunia:Ayan_Kumar.html" target="_blank"><font size="3">  DBLP  </font></a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ayancircle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ayancircle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--            <tr>-->
<!--            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">-->
<!--              <heading><font size="5">Research Interests</font></heading>-->
<!--              <p><font size="3">-->
<!--                I am broadly interested in the field of Computer Vision and Deep Learning. Particularly, I like to think upon <b>Visual Scene Understanding (VSU)</b> from images and videos, effective methods of <b>Domain Adaptation & Transfer Learning</b> for VSU, building <b>systems that learn with minimal or no supervision</b> and <b>systems that generalize well</b> in real and diverse scenarios.-->
<!--                I am also open to any topic that would be interesting or fun to explore and pursue.-->
<!--              </font></p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->

        <!-- PUBLICATIONS -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle;text-align:justify">
               <p>
              </p>
              <heading><font size="5">Selected Publications</font></heading>
            </td>
          </tr>
        </tbody></table>

        <script>
          function myFunction(pub_name) {
              var x = document.getElementById(pub_name);
              if (x.style.display === 'none') {
                  x.style.display = 'block';
              } else {
                  x.style.display = 'none';
              }
        }
        </script>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


                  <!-- CVPR 2019 - HANDWRITING RECOGNITION -->
          <tr onmouseout="cvpr20_0_stop()" onmouseover="cvpr20_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr20_0_image'><img src='images/cvpr2020_1.jpg'></div>
                <img src='images/cvpr2020_2.jpg'>
              </div>
              <script type="text/javascript">
                function cvpr20_0_start() {
                  document.getElementById('cvpr20_0_image').style.opacity = "1";
                }

                function cvpr20_0_stop() {
                  document.getElementById('cvpr20_0_image').style.opacity = "0";
                }
                cvpr20_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2002.10310" target="_blank">
                <papertitle><font size="3">Sketch Less for More: On-the-Fly Fine-Grained Sketch Based Image Retrieval</font></papertitle>
              </a>
              <br>
               <strong><font size="3"> Ayan Kumar Bhunia</font></strong>, <font size="3">Yongxin Yang</font>, <font size="3">Timothy M. Hospedalis</font>,
              <font size="3">Tao Xiang</font>, <font size="3">Yi-Zhe Song</font>.
              <br><font size="3">
              <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong><font size="3.5">CVPR</font></strong>)</em>, 2020.

              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('cvpr20_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://github.com/AyanKumarBhunia/on-the-fly-FGSBIR" target="_blank"><font size="3">Code</font></a> /
              <a href="https://arxiv.org/abs/2002.10310" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('cvpr20_0_bib')"><font size="3">BibTex</font></a>
              <font color="red"><strong>(Oral Presentation)</strong></font>
              <p></p>
              <div id="cvpr20_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>Fine-grained sketch-based image retrieval (FG-SBIR) addresses the problem of retrieving a particular photo instance given a user's query sketch. Its widespread applicability is however hindered by the fact that drawing a sketch takes time, and most people struggle to draw a complete and faithful sketch. In this paper, we reformulate the conventional FG-SBIR framework to tackle these challenges, with the ultimate goal of retrieving the target photo with the least number of strokes possible. We further propose an on-the-fly design that starts retrieving as soon as the user starts drawing. To accomplish this, we devise a reinforcement learning based cross-modal retrieval framework that directly optimizes rank of the ground-truth photo over a complete sketch drawing episode. Additionally, we introduce a novel reward scheme that circumvents the problems related to irrelevant sketch strokes, and thus provides us with a more consistent rank list during the retrieval. We achieve superior early-retrieval efficiency over state-of-the-art methods and alternative baselines on two publicly available fine-grained sketch retrieval datasets.
                </em>
              </font></div>
              <div id="cvpr20_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{bhunia2020sketch,<br>
                author = {Ayan Kumar Bhunia and Yongxin Yang and Timothy M. Hospedales and Tao Xiang and Yi-Zhe Song},<br>
                title = {Sketch Less for More: On-the-Fly Fine-Grained Sketch Based Image Retrieval},<br>
                booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
                month = {June},<br>
                year = {2020}<br>
                }
              </font></div>
              </td>
          </tr>


          <!-- CVPR 2019 - HANDWRITING RECOGNITION -->
          <tr onmouseout="cvpr19_0_stop()" onmouseover="cvpr19_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr19_0_image'><img src='images/cvpr2019_2.JPG'></div>
                <img src='images/cvpr2019_1.JPG'>
              </div>
              <script type="text/javascript">
                function cvpr19_0_start() {
                  document.getElementById('cvpr19_0_image').style.opacity = "1";
                }

                function cvpr19_0_stop() {
                  document.getElementById('cvpr19_0_image').style.opacity = "0";
                }
                cvpr19_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Bhunia_Handwriting_Recognition_in_Low-Resource_Scripts_Using_Adversarial_Learning_CVPR_2019_paper.html" target="_blank">
                <papertitle><font size="3">Handwriting Recognition in Low-Resource Scripts Using Adversarial Learning</font></papertitle>
              </a>
              <br>

               <strong><font size="3"> Ayan Kumar Bhunia </font></strong>, <font size="3">Abhirup Das</font>, <font size="3">Ankan Kumar Bhunia</font>,
              <font size="3">Perla Sai Raj Kishore</font>, <font size="3">Partha Pratim Roy</font>.
              <br><font size="3">
              <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong><font size="3.5">CVPR</font></strong>)</em>, 2019
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('cvpr19_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://github.com/AyanKumarBhunia/Handwriting_Recogition_using_Adversarial_Learning" target="_blank"><font size="3">Code</font></a> /
              <a href="https://arxiv.org/abs/1811.01396" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('cvpr19_0_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="cvpr19_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Handwritten Word Recognition and Spotting is a challenging field dealing with handwritten text possessing irregular and complex shapes. The design of deep neural network models makes it necessary to extend training datasets in order to introduce variations and increase the number of samples; word-retrieval is therefore very difficult in low-resource scripts. Much of the existing literature comprises preprocessing strategies which are seldom sufficient to cover all possible variations. We propose an Adversarial Feature Deformation Module (AFDM) that learns ways to elastically warp extracted features in a scalable manner. The AFDM is inserted between intermediate layers and trained alternatively with the original framework, boosting its capability to better learn highly informative features rather than trivial ones. We test our meta-framework, which is built on top of popular word-spotting and word-recognition frameworks and enhanced by AFDM, not only on extensive Latin word datasets but also on sparser Indic scripts. We record results for varying sizes of training data, and observe that our enhanced network generalizes much better in the low-data regime; the overall word-error rates and mAP scores are observed to improve as well.
                </em>
              </font></div>
              <div id="cvpr19_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{Bhunia_2019_CVPR,<br>
                author = {Bhunia, Ayan Kumar and Das, Abhirup and Bhunia, Ankan Kumar and Kishore, Perla Sai Raj and Roy, Partha Pratim},<br>
                title = {Handwriting Recognition in Low-Resource Scripts Using Adversarial Learning},<br>
                booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
                month = {June},<br>
                year = {2019}<br>
                }
              </font></div>
              </td>
          </tr>


          <!-- ICIP 2019 - Unsupervised Binarization -->
          <tr onmouseout="ICIP19_0_stop()" onmouseover="ICIP19_0_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ICIP19_0_image'><img src='images/ICIP219_1.JPG'></div>
                <img src='images/ICIP219_2.JPG'>
              </div>
              <script type="text/javascript">
                function ICIP19_0_start() {
                  document.getElementById('ICIP19_0_image').style.opacity = "1";
                }
                function ICIP19_0_stop() {
                  document.getElementById('ICIP19_0_image').style.opacity = "0";
                }
                ICIP19_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8803348/" target="_blank">
                <papertitle><font size="3">Improving Document Binarization via Adversarial Noise-Texture Augmentation</font></papertitle>
              </a>
              <br>

               <font size="3">Ankan Kumar Bhunia</font> <strong><font size="3"> Ayan Kumar Bhunia </font></strong>,
              <font size="3">Aneeshan Sain</font>, <font size="3">Partha Pratim Roy</font>.
              <br><font size="3">
              <em>IEEE Conference on Image Processing (<strong><font size="3.5">ICIP</font></strong>)</em>, 2019
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('ICIP19_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://github.com/ankanbhunia/AdverseBiNet" target="_blank"><font size="3">Code</font></a> /
              <a href="https://arxiv.org/abs/1810.11120" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('ICIP19_0_bib')"><font size="3">BibTex</font></a>
              <font color="red"><strong>(Top 10% Papers)</strong></font>
              <p></p>
              <div id="ICIP19_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Binarization of degraded document images is an elementary step in most of the problems in document image analysis domain. The paper re-visits the binarization problem by introducing an adversarial learning approach. We construct a Texture Augmentation Network that transfers the texture element of a degraded reference document image to a clean binary image. In this way, the network creates multiple versions of the same textual content with various noisy textures, thus enlarging the available document binarization datasets. At last, the newly generated images are passed through a Binarization network to get back the clean version. By jointly training the two networks we can increase the adversarial robustness of our system. Also, it is noteworthy that our model can learn from unpaired data. Experimental results suggest that the proposed method achieves superior performance over widely used DIBCO datasets.
                </em>
              </font></div>
              <div id="ICIP19_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @inproceedings{bhunia2019improving, <br>
                title={Improving document binarization via adversarial noise-texture augmentation},
                author={Bhunia, Ankan Kumar and Bhunia, Ayan Kumar and Sain, Aneeshan and Roy, Partha Pratim},
                booktitle={2019 IEEE International Conference on Image Processing (ICIP)},
                pages={2721--2725},<br>
                year={2019},<br>
                organization={IEEE}
                }
              </font></div>
              </td>
          </tr>

<!--                    &lt;!&ndash; CVPR 2019 - HANDWRITING RECOGNITION &ndash;&gt;-->
<!--          <tr onmouseout="cvpr19_0_stop()" onmouseover="cvpr19_0_start()">-->
<!--            <td style="padding:0px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='cvpr19_0_image'><img src='images/BarronCVPR2012_before.jpg'></div>-->
<!--                <img src='images/BarronCVPR2012_before.jpg'>-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function cvpr19_0_start() {-->
<!--                  document.getElementById('cvpr19_0_image').style.opacity = "1";-->
<!--                }-->

<!--                function cvpr19_0_stop() {-->
<!--                  document.getElementById('cvpr19_0_image').style.opacity = "0";-->
<!--                }-->
<!--                cvpr19_0_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Bhunia_Handwriting_Recognition_in_Low-Resource_Scripts_Using_Adversarial_Learning_CVPR_2019_paper.html" target="_blank">-->
<!--                <papertitle><font size="3">Handwriting Recognition in Low-Resource Scripts Using Adversarial Learning</font></papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--               <strong><font size="3"> Ayan Kumar Bhunia </font></strong>, <font size="3">Abhirup Das</font>, <font size="3">Ankan Kumar Bhunia</font>,-->
<!--              <font size="3">Perla Sai Raj Kishore</font>, <font size="3">Partha Pratim Roy</font>.-->
<!--              <br><font size="3">-->
<!--              <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong><font size="3.5">CVPR</font></strong>)</em>, 2019-->
<!--              </font>-->
<!--              <br>-->
<!--              <p></p>-->
<!--              <a href="javascript:void(0);" onclick="myFunction('cvpr19_0_abs')"><font size="3">Abstract</font></a> /-->
<!--              <a href="https://github.com/AyanKumarBhunia/Handwriting_Recogition_using_Adversarial_Learning" target="_blank"><font size="3">Code</font></a> /-->
<!--              <a href="https://arxiv.org/abs/1811.01396" target="_blank" target="_blank"><font size="3">arXiv</font></a> /-->
<!--              <a href="javascript:void(0);" onclick="myFunction('cvpr19_0_bib')"><font size="3">BibTex</font></a>-->
<!--              <p></p>-->
<!--              <div id="cvpr19_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">-->
<!--                <em>-->
<!--                  Handwritten Word Recognition and Spotting is a challenging field dealing with handwritten text possessing irregular and complex shapes. The design of deep neural network models makes it necessary to extend training datasets in order to introduce variations and increase the number of samples; word-retrieval is therefore very difficult in low-resource scripts. Much of the existing literature comprises preprocessing strategies which are seldom sufficient to cover all possible variations. We propose an Adversarial Feature Deformation Module (AFDM) that learns ways to elastically warp extracted features in a scalable manner. The AFDM is inserted between intermediate layers and trained alternatively with the original framework, boosting its capability to better learn highly informative features rather than trivial ones. We test our meta-framework, which is built on top of popular word-spotting and word-recognition frameworks and enhanced by AFDM, not only on extensive Latin word datasets but also on sparser Indic scripts. We record results for varying sizes of training data, and observe that our enhanced network generalizes much better in the low-data regime; the overall word-error rates and mAP scores are observed to improve as well.-->
<!--                </em>-->
<!--              </font></div>-->
<!--              <div id="cvpr19_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">-->
<!--                <br>-->
<!--                @InProceedings{Bhunia_2019_CVPR,<br>-->
<!--                author = {Bhunia, Ayan Kumar and Das, Abhirup and Bhunia, Ankan Kumar and Kishore, Perla Sai Raj and Roy, Partha Pratim},<br>-->
<!--                title = {Handwriting Recognition in Low-Resource Scripts Using Adversarial Learning},<br>-->
<!--                booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>-->
<!--                month = {June},<br>-->
<!--                year = {2019}<br>-->
<!--                }-->
<!--              </font></div>-->
<!--              </td>-->
<!--          </tr>-->



        <!-- Footer - Template Credits -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Template credits :
                <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
